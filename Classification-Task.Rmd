---
title: "Classification-Task"
author: "JoshuaGK"
date: "2023-11-19"
output: html_document
---
```{r}
# install.packages("rBayesianOptimization")
library(randomForest)
library(e1071)
library(rBayesianOptimization)
library(pROC)
library(ROCR)
library(ranger)
library(xgboost)
library(mice)
```

First we load the data
```{r}
useImputed <- TRUE

if (useImputed) {
  data.not_test <- read.csv("imputed_data/imputed_data_1.csv")
  data.test <- read.csv("cs-test.csv")
  # Remove the index column so we don't overfit
  column_to_remove <- "X"
  column_to_remove_2 <- "SeriousDlqin2yrs"
  data.not_test <- data.not_test[,-which(names(data.not_test) == column_to_remove)]
  data.test <- data.test[,-which(names(data.test) == column_to_remove)]
  data.test <- data.test[,-which(names(data.test) == column_to_remove_2)]
  
  print(dim(data.not_test))
  print(dim(data.test))
  
  # Next we remove the na values
  data.not_test <- na.omit(data.not_test)
  data.test <- na.omit(data.test)
  
  print(dim(data.not_test))
  print(dim(data.test))
} else {
  data.not_test <- read.csv("cs-training.csv")
  data.test <- read.csv("cs-test.csv")
  # Remove the index column so we don't overfit
  column_to_remove <- "X"
  column_to_remove_2 <- "SeriousDlqin2yrs"
  data.not_test <- data.not_test[,-which(names(data.not_test) == column_to_remove)]
  data.test <- data.test[,-which(names(data.test) == column_to_remove)]
  data.test <- data.test[,-which(names(data.test) == column_to_remove_2)]
  
  print(dim(data.not_test))
  print(dim(data.test))
  
  # Next we remove the na values
  data.not_test <- na.omit(data.not_test)
  data.test <- na.omit(data.test)
  
  print(dim(data.not_test))
  print(dim(data.test))
}

```
For the purpose of testing, we will use a much smaller size of the data
```{r}
sandbox_size <- 0.05
num_rows <- nrow(data.not_test)
data.not_test <- data.not_test[1:round(sandbox_size*num_rows),]
num_rows_test <- nrow(data.test)
data.test <- data.test[1:round(sandbox_size*num_rows_test),]
print(dim(data.not_test))
print(dim(data.test))
```

Next we split the data into train and validation sets.
```{r}
train_size = 0.7
split <- round(nrow(data.not_test)*train_size)
num_rows <- nrow(data.not_test)

data.train <- data.not_test[1:split,]
data.train_labels <- data.not_test[1:split,which(names(data.train) == column_to_remove_2)]
data.train_no_labels <- data.not_test[1:split,-which(names(data.train) == column_to_remove_2)]
data.cv <- data.not_test[(split+1):num_rows,] # For some reason, matrix splicing is inclusive on both sides [start, end] not [start, end)

print(dim(data.train))
print(dim(data.cv))
```
Now we create a function to get the accuracy of our model on the test_data
```{r}
get_accuracy <- function(model, test_data) {
  prediction <- predict(model, test_data)
  confusion_matrix <- table(prediction, test_data$SeriousDlqin2yrs)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  return (accuracy)
}
```

Now we want to train a decision tree classifier using our cross validation set
```{r}
rf_model <- randomForest(factor(SeriousDlqin2yrs) ~ ., data = data.train, ntree=500)

accuracy <- get_accuracy(rf_model, data.cv)
print(accuracy)

```
Now we create an svm classifier
```{r}
svm_model <- svm(factor(SeriousDlqin2yrs) ~ ., data=data.train, kernel="radial", cost = 4, gamma = 0.5)
accuracy_svm <- get_accuracy(svm_model, data.cv)
print(accuracy_svm)
```
Now we test out the bayesian optimization package:
```{r}
# print(data.train_labels)
data <- data.train_no_labels
y <- data.train_labels #droplevels(iris[51:150,5])
b <- sample(1:100) 
data <- data[b,]
y <- y[b]
print("aaaa")
print(y)
print("aaaa")

svm.cv1 <- function(gamma, cost) {
  n <- nrow(data)
  splits <- c(seq(1,n, n/10),n+1)
  jumps <- n/10
  fitted <- c()
  for (i in 1:10) {
    test <- splits[i:(i+1)]
    test[2] <- test[2]-1
    test.y <- as.factor(y[test[1]: test[2]])
    # print(test.y)
    test.data <- data.frame(y=test.y, data[test[1]:test[2],])
    
    train <- (1:n)[-(test[1]:test[2])]
    train.y <- as.factor(y[-(test[1]:test[2])])
    train.data <- data.frame(y=train.y,data[train,])

    fit <- svm(y~., data=train.data, gamma=gamma, cost=cost)
    fitted <- c(fitted, attributes(predict(fit, test.data, decision.values=TRUE))$decision)
    print(y)
    # print(length(y))
    # print(length(fitted))
    # print(attributes(predict(fit, test.data, decision.values=TRUE)))
  }
  return(list(Score=roc(y,fitted)$auc, Pred=0))
}
# test <- svm.cv1(0.377, 508)
bayesian_optimized_svm <- BayesianOptimization(svm.cv1, bounds=list(gamma=c(0,2), cost=c(0,1000)), init_points=10, n_iter=20 )
```


```{r}
# Now we make a svm based on the parameters we got from above
best_params <- bayesian_optimized_svm$Best_Par
print(best_params)
best_svm <- svm(factor(SeriousDlqin2yrs) ~ ., data=data.train, kernel="radial", cost = 0.7752159, gamma = 865.7703376)
accuracy_svm <- get_accuracy(best_svm, data.cv)
print(accuracy_svm)
```

```{r}
# print(data.train_labels)
data <- data.train_no_labels
y <- data.train_labels #droplevels(iris[51:150,5])
b <- sample(1:100) 
data <- data[b,]
y <- y[b]

rf_objective <- function(ntree, mtry) {
  n <- nrow(data)
  splits <- c(seq(1,n, n/10),n+1)
  jumps <- n/10
  fitted <- c()
  for (i in 1:10) {
    test <- splits[i:(i+1)]
    test[2] <- test[2]-1
    test.y <- as.factor(y[test[1]: test[2]])
    # print(test.y)
    test.data <- data.frame(y=test.y, data[test[1]:test[2],])
    
    train <- (1:n)[-(test[1]:test[2])]
    train.y <- as.factor(y[-(test[1]:test[2])])
    train.data <- data.frame(y=train.y,data[train,])

    # fit <- randomForest(y~., data=train.data, ntree=ntree, mtry=mtry)
    # pred <- predict(fit, test.data, type="response")
    # pred_list <- as.numeric(as.character(pred))
    
    fit <- ranger(y~., data=train.data, num.trees=ntree, mtry=mtry)
    pred <- predict(fit, test.data, type="response")
    pred_list <- as.numeric(as.character(pred$predictions))
    # print("=======")
    # print(as.numeric(as.character(pred$predictions)))
    # print("=======")
    fitted <- c(fitted, pred_list)
  }
  return (list(Score=roc(y,fitted)$auc, Pred=0))
}
# t <- rf_objective(500, 2)
bayesian_optimized_rf <- BayesianOptimization(FUN=rf_objective, bounds=list(ntree=c(500, 750), mtry=c(1,4)), init_points=10, n_iter=20)
```
As you can see above, randomForests from the ranger class as a very poor AUC. 

We will now try the xgboost package with bayesian optimization
```{r}
data <- data.train_no_labels
y <- data.train_labels #droplevels(iris[51:150,5])
b <- sample(1:100) 
data <- data[b,]
y <- y[b]

xgboost_objective <- function(eta) {
  n <- nrow(data)
  splits <- c(seq(1,n, n/10),n+1)
  jumps <- n/10
  fitted <- c()
  for (i in 1:10) {
    test <- splits[i:(i+1)]
    test[2] <- test[2]-1
    test.y <- as.factor(y[test[1]: test[2]])
    # print(test.y)
    test.data <- data.frame(y=test.y, data[test[1]:test[2],])
    
    # print("test.y")
    # print(as.numeric(as.character(test.y)))
    
    train <- (1:n)[-(test[1]:test[2])]
    train.y <- as.factor(y[-(test[1]:test[2])])
    train.data <- data.frame(y=train.y,data[train,])
    
    # print("train.y")
    # print(as.numeric(as.character(train.y)))
    
    # Convert the data to xgboost format
    train_matrix <- xgb.DMatrix(data=as.matrix(data[train,]), label=as.numeric(as.character(train.y)))
    test_matrix <- xgb.DMatrix(data=as.matrix(data[test[1]:test[2],], label=as.numeric(as.character(test.y))))
    params <- list(
      objective = "binary:logistic",
      eval_metric = "logloss",
      max_depth = 40,
      eta = eta,
      nthread = 4
    )
    
    fit <- xgboost(data = train_matrix, params = params, nrounds = 50, verbose = 0)
    pred <- predict(fit, test_matrix)
    pred_list <- ifelse(pred>0.5, 1, 0)
    # print("=======")
    # print(as.numeric(as.character(pred$predictions)))
    # print(pred_list)
    # print("=======")
    fitted <- c(fitted, pred_list)
  }
  return (list(Score=roc(y,fitted)$auc, Pred=0))
}
# t <- xgboost_objective(500, 2)
bayesian_optimized_rf <- BayesianOptimization(FUN=xgboost_objective, bounds=list(eta=c(0.0001,1)), init_points=100, n_iter=20)
```
For some reason, the above model does not get above "0.63"

Since we found that the svm model did the best on our training and cross validation sets, we decided to use this with our imputed data 

Using the imputed data:

Mo's second attempt at getting probabilities:

```{r}
library(mice)
library(e1071)

# Load data
data.train <- read.csv("cs-training.csv")
data.test <- read.csv("cs-test.csv")
data.test$SeriousDlqin2yrs <- NULL

# Parameters
num_imp <- 11
gamma <- 1.403102
cost <- 3.659304
subset_size <- 0.10 # Using first 10% of the training data

# Impute data
set.seed(47)
imputed_data_train <- mice(data.train, m=num_imp, method='pmm', printFlag=FALSE)
imputed_data_test <- mice(data.test, m=num_imp, method='pmm', printFlag=FALSE)
```
Now, to train an svm on each of these imputed sets, and generate a probability of deliquincy for each, then average those probabilities.

```{r}
# Initialize a list to store probability values from all imputations
probability_values_list <- vector("list", num_imp)

# Loop through each imputation
for(i in 1:num_imp) {
    # Get the ith imputed dataset
    imputed_train <- complete(imputed_data_train, i)
    imputed_test <- complete(imputed_data_test, i)

    # Subset the training data
    subset_train <- imputed_train[1:(nrow(imputed_train) * subset_size), ]

    # Train the SVM model with probability estimates
    svm_model <- svm(factor(SeriousDlqin2yrs) ~ . - X, data = subset_train, probability = TRUE, gamma = gamma, cost = cost)

    # Predict probabilities for ith imputation
    svm_prediction <- predict(svm_model, imputed_test, probability = TRUE)
    
    # Extract probabilities
    prob_values <- attr(svm_prediction, "probabilities")

    # Check if the structure is as expected, and extract the relevant probabilities
    if (!is.null(prob_values) && ncol(prob_values) == 2) {
        positive_class_probabilities <- prob_values[, 1]
        probability_values_list[[i]] <- positive_class_probabilities
    } else {
        warning("Unexpected structure in SVM probability output for imputation ", i)
        next  # Skip this imputation
    }
}

# Convert list to matrix and calculate average probability
average_probabilities_matrix <- do.call(cbind, probability_values_list)
average_probabilities <- rowMeans(average_probabilities_matrix, na.rm = TRUE)
```

```{r}
# Create a data frame with Id and Probability columns
data <- data.frame(
  Id = 1:length(average_probabilities),
  Probability = average_probabilities
)

# Create a filename string with gamma and cost values
filename <- paste0("output_gamma_", gamma, "_cost_", cost, "_imp_", num_imp, ".csv")

# Write the data frame to a CSV file with the new filename
write.csv(data, filename, row.names = FALSE)
```


