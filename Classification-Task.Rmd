---
title: "Classification-Task"
author: "JoshuaGK"
date: "2023-11-19"
output: html_document
---
```{r}
# install.packages("rBayesianOptimization")
library(randomForest)
library(e1071)
library(rBayesianOptimization)
library(pROC)
library(ROCR)
library(ranger)
library(xgboost)
library(mice)
```

First we load the data
```{r}
useImputed <- TRUE

if (useImputed) {
  num_imp = 3
  data.not_test <- read.csv("cs-training.csv")
  data.test <- read.csv("cs-test.csv")
  # Remove the index column so we don't overfit
  column_to_remove <- "X"
  column_to_remove_2 <- "SeriousDlqin2yrs"
  
  data.not_test <- data.not_test[,-which(names(data.not_test) == column_to_remove)]
  data.imputed.not_test <- mice(data.not_test, m=num_imp, method='pmm', seed=47, printFlag=FALSE)
  data.not_test <- complete(data.imputed.not_test, 1)
  
  data.test <- data.test[,-which(names(data.test) == column_to_remove)]
  data.test <- data.test[,-which(names(data.test) == column_to_remove_2)]
  data.imputed.test <- mice(data.test, model=data.imputed.not_test)
  data.test <- complete(data.imputed.test, 1)
  data.test.full <- complete(data.imputed.test, 1)
  
  print(dim(data.not_test))
  print(dim(data.test))
} else {
  data.not_test <- read.csv("cs-training.csv")
  data.test <- read.csv("cs-test.csv")
  # Remove the index column so we don't overfit
  column_to_remove <- "X"
  column_to_remove_2 <- "SeriousDlqin2yrs"
  data.not_test <- data.not_test[,-which(names(data.not_test) == column_to_remove)]
  data.test <- data.test[,-which(names(data.test) == column_to_remove)]
  data.test <- data.test[,-which(names(data.test) == column_to_remove_2)]
  
  print(dim(data.not_test))
  print(dim(data.test))
  
  # Next we remove the na values
  data.not_test <- na.omit(data.not_test)
  data.test <- na.omit(data.test)
  
  print(dim(data.not_test))
  print(dim(data.test))
}

```
For the purpose of testing, we will use a much smaller size of the data
```{r}
set.seed(47)
sandbox_size <- 0.05
num_rows <- nrow(data.not_test)
data.not_test <- data.not_test[1:round(sandbox_size*num_rows),]
data.not_test <- sample(nrow(data.not_test))
num_rows_test <- nrow(data.test)
data.test <- data.test[1:round(sandbox_size*num_rows_test),]
data.test <- sample(nrow(data.test))
print(dim(data.not_test))
print(dim(data.test))
```

Next we split the data into train and validation sets.
```{r}
train_size = 0.7
split <- round(nrow(data.not_test)*train_size)
num_rows <- nrow(data.not_test)

data.train <- data.not_test[1:split,]
data.train_labels <- data.not_test[1:split,which(names(data.train) == column_to_remove_2)]
data.train_no_labels <- data.not_test[1:split,-which(names(data.train) == column_to_remove_2)]
data.cv <- data.not_test[(split+1):num_rows,] # For some reason, matrix splicing is inclusive on both sides [start, end] not [start, end)

print(dim(data.train))
print(dim(data.cv))
```
Now we create a function to get the accuracy of our model on the test_data
```{r}
get_accuracy <- function(model, test_data) {
  prediction <- predict(model, test_data)
  confusion_matrix <- table(prediction, test_data$SeriousDlqin2yrs)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  return (accuracy)
}
```

Now we want to train a decision tree classifier using our cross validation set
```{r}
rf_model <- randomForest(factor(SeriousDlqin2yrs) ~ ., data = data.train, ntree=500)

accuracy <- get_accuracy(rf_model, data.cv)
print(accuracy)

```
Now we create an svm classifier
```{r}
svm_model <- svm(factor(SeriousDlqin2yrs) ~ ., data=data.train, kernel="radial", cost = 4, gamma = 0.5)
accuracy_svm <- get_accuracy(svm_model, data.cv)
print(accuracy_svm)
```
Now we test out the bayesian optimization package:
```{r}
# print(data.train_labels)
data <- data.train_no_labels
y <- data.train_labels 
b <- sample(1:length(y)) 
data <- data[b,]
y <- y[b]
y <- as.factor(y)

svm.cv1 <- function(gamma, cost) {
  n <- nrow(data)
  splits <- c(seq(1,n, n/10),n+1)
  jumps <- n/10
  fitted <- c()
  for (i in 1:10) {
    test <- splits[i:(i+1)]
    test[2] <- test[2]-1
    test.y <- as.factor(y[test[1]: test[2]])
    # print(test.y)
    test.data <- data.frame(y=test.y, data[test[1]:test[2],])
    
    train <- (1:n)[-(test[1]:test[2])]
    train.y <- as.factor(y[-(test[1]:test[2])])
    train.data <- data.frame(y=train.y,data[train,])

    fit <- svm(y~., data=train.data, kernel="radial", gamma=gamma, cost=cost, probability=TRUE)
    predictions <- predict(fit, test.data, decision.values=TRUE, probability=TRUE)
    decisions_attr <- attributes(predictions)$decision
    # [, 1] corresponds to the column predicting 0 and 1 corresponds to the column predicting 1
    probabilities <- as.numeric(as.character(attributes(predictions)$probabilities[,1]))
    # decisions <- as.numeric(as.character(predictions))
    # decisions <- ifelse(decisions > 0, 0, 1)
    fitted <- c(fitted, probabilities)
    # print("Y")
    # print(y)
    # print("probabilities")
    # print(probabilities)
    # print("predictions")
    # print(decisions)
    # print("decisions")
    # print(decisions_attr)
    # print("attributes")
    # print(attributes(predictions))
    # print(as.numeric(as.character(predictions)))
    # print(attributes(predict(fit, test.data, decision.values=TRUE))$decision)
    # print(length(y))
    # print(length(fitted))
    # print(attributes(predict(fit, test.data, decision.values=TRUE)))
  }
  # print("fitted")
  # print(fitted)
  return(list(Score=roc(y,fitted)$auc, Pred=0))
}
# test <- svm.cv1(0.377, 508)
bayesian_optimized_svm <- BayesianOptimization(svm.cv1, bounds=list(gamma=c(0,5), cost=c(0,50)), init_points=20, n_iter=20 )
```


```{r}
# Now we make a svm based on the parameters we got from above
best_params <- bayesian_optimized_svm$Best_Par
print(as.character(best_params["gamma"]))
best_cost <- as.numeric(as.character(best_params["cost"]))
best_gamma <- as.numeric(as.character(best_params["gamma"]))
print(best_cost)
print(best_gamma)
best_svm <- svm(factor(SeriousDlqin2yrs) ~ ., data=data.train, kernel="radial", cost = best_cost, gamma = best_gamma, probability=TRUE)
accuracy_svm <- get_accuracy(best_svm, data.cv)
print(accuracy_svm)
```
As you can see from above, our best ROC value was 0.9139785 using svm with a radial kernel. The parameters to achieve this are gamma = 9.408567e-05 and cost = 1232.3198. 

Round = 28	gamma = 8.930776e-05	cost = 1342.2408	Value = 0.8540707

```{r}
predictions <- predict(best_svm, data.test.full, decision.values=TRUE, probability=TRUE)
decisions <-  attributes(predictions)$decision
probabilities <- attributes(predictions)$probabilities
prob_of_1 <- probabilities[,1]
print(decisions)
```

```{r}
write.csv(data.frame(probabilities=prob_of_1), "model_predictions.csv")
```


