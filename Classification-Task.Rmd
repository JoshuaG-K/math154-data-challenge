---
title: "Classification-Task"
author: "JoshuaGK"
date: "2023-11-19"
output: html_document
---
```{r}
# install.packages("rBayesianOptimization")
library(randomForest)
library(e1071)
library(rBayesianOptimization)
library(pROC)
library(ROCR)
library(ranger)
library(xgboost)
library(mice)
```

First we load the data
```{r}
useImputed <- TRUE

if (useImputed) {
  data.not_test <- read.csv("imputed_data/imputed_data_1.csv")
  data.test <- read.csv("cs-test.csv")
  # Remove the index column so we don't overfit
  column_to_remove <- "X"
  column_to_remove_2 <- "SeriousDlqin2yrs"
  data.not_test <- data.not_test[,-which(names(data.not_test) == column_to_remove)]
  data.test <- data.test[,-which(names(data.test) == column_to_remove)]
  data.test <- data.test[,-which(names(data.test) == column_to_remove_2)]
  
  print(dim(data.not_test))
  print(dim(data.test))
  
  # Next we remove the na values
  data.not_test <- na.omit(data.not_test)
  data.test <- na.omit(data.test)
  
  print(dim(data.not_test))
  print(dim(data.test))
} else {
  data.not_test <- read.csv("cs-training.csv")
  data.test <- read.csv("cs-test.csv")
  # Remove the index column so we don't overfit
  column_to_remove <- "X"
  column_to_remove_2 <- "SeriousDlqin2yrs"
  data.not_test <- data.not_test[,-which(names(data.not_test) == column_to_remove)]
  data.test <- data.test[,-which(names(data.test) == column_to_remove)]
  data.test <- data.test[,-which(names(data.test) == column_to_remove_2)]
  
  print(dim(data.not_test))
  print(dim(data.test))
  
  # Next we remove the na values
  data.not_test <- na.omit(data.not_test)
  data.test <- na.omit(data.test)
  
  print(dim(data.not_test))
  print(dim(data.test))
}

```
For the purpose of testing, we will use a much smaller size of the data
```{r}
sandbox_size <- 0.05
num_rows <- nrow(data.not_test)
data.not_test <- data.not_test[1:round(sandbox_size*num_rows),]
num_rows_test <- nrow(data.test)
data.test <- data.test[1:round(sandbox_size*num_rows_test),]
print(dim(data.not_test))
print(dim(data.test))
```

Next we split the data into train and validation sets.
```{r}
train_size = 0.7
split <- round(nrow(data.not_test)*train_size)
num_rows <- nrow(data.not_test)

data.train <- data.not_test[1:split,]
data.train_labels <- data.not_test[1:split,which(names(data.train) == column_to_remove_2)]
data.train_no_labels <- data.not_test[1:split,-which(names(data.train) == column_to_remove_2)]
data.cv <- data.not_test[(split+1):num_rows,] # For some reason, matrix splicing is inclusive on both sides [start, end] not [start, end)

print(dim(data.train))
print(dim(data.cv))
```
Now we create a function to get the accuracy of our model on the test_data
```{r}
get_accuracy <- function(model, test_data) {
  prediction <- predict(model, test_data)
  confusion_matrix <- table(prediction, test_data$SeriousDlqin2yrs)
  accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
  return (accuracy)
}
```

Now we want to train a decision tree classifier using our cross validation set
```{r}
rf_model <- randomForest(factor(SeriousDlqin2yrs) ~ ., data = data.train, ntree=500)

accuracy <- get_accuracy(rf_model, data.cv)
print(accuracy)

```
Now we create an svm classifier
```{r}
svm_model <- svm(factor(SeriousDlqin2yrs) ~ ., data=data.train, kernel="radial", cost = 4, gamma = 0.5)
accuracy_svm <- get_accuracy(svm_model, data.cv)
print(accuracy_svm)
```
Now we test out the bayesian optimization package:
```{r}
# print(data.train_labels)
data <- data.train_no_labels
y <- data.train_labels #droplevels(iris[51:150,5])
b <- sample(1:100) 
data <- data[b,]
y <- y[b]
print("aaaa")
print(y)
print("aaaa")

svm.cv1 <- function(gamma, cost) {
  n <- nrow(data)
  splits <- c(seq(1,n, n/10),n+1)
  jumps <- n/10
  fitted <- c()
  for (i in 1:10) {
    test <- splits[i:(i+1)]
    test[2] <- test[2]-1
    test.y <- as.factor(y[test[1]: test[2]])
    # print(test.y)
    test.data <- data.frame(y=test.y, data[test[1]:test[2],])
    
    train <- (1:n)[-(test[1]:test[2])]
    train.y <- as.factor(y[-(test[1]:test[2])])
    train.data <- data.frame(y=train.y,data[train,])

    fit <- svm(y~., data=train.data, gamma=gamma, cost=cost)
    fitted <- c(fitted, attributes(predict(fit, test.data, decision.values=TRUE))$decision)
    print(y)
    # print(length(y))
    # print(length(fitted))
    # print(attributes(predict(fit, test.data, decision.values=TRUE)))
  }
  return(list(Score=roc(y,fitted)$auc, Pred=0))
}
# test <- svm.cv1(0.377, 508)
bayesian_optimized_svm <- BayesianOptimization(svm.cv1, bounds=list(gamma=c(0,2), cost=c(0,1000)), init_points=10, n_iter=20 )
```


```{r}
# Now we make a svm based on the parameters we got from above
best_params <- bayesian_optimized_svm$Best_Par
print(best_params)
best_svm <- svm(factor(SeriousDlqin2yrs) ~ ., data=data.train, kernel="radial", cost = 0.7752159, gamma = 865.7703376)
accuracy_svm <- get_accuracy(best_svm, data.cv)
print(accuracy_svm)
```

```{r}
# print(data.train_labels)
data <- data.train_no_labels
y <- data.train_labels #droplevels(iris[51:150,5])
b <- sample(1:100) 
data <- data[b,]
y <- y[b]

rf_objective <- function(ntree, mtry) {
  n <- nrow(data)
  splits <- c(seq(1,n, n/10),n+1)
  jumps <- n/10
  fitted <- c()
  for (i in 1:10) {
    test <- splits[i:(i+1)]
    test[2] <- test[2]-1
    test.y <- as.factor(y[test[1]: test[2]])
    # print(test.y)
    test.data <- data.frame(y=test.y, data[test[1]:test[2],])
    
    train <- (1:n)[-(test[1]:test[2])]
    train.y <- as.factor(y[-(test[1]:test[2])])
    train.data <- data.frame(y=train.y,data[train,])

    # fit <- randomForest(y~., data=train.data, ntree=ntree, mtry=mtry)
    # pred <- predict(fit, test.data, type="response")
    # pred_list <- as.numeric(as.character(pred))
    
    fit <- ranger(y~., data=train.data, num.trees=ntree, mtry=mtry)
    pred <- predict(fit, test.data, type="response")
    pred_list <- as.numeric(as.character(pred$predictions))
    # print("=======")
    # print(as.numeric(as.character(pred$predictions)))
    # print("=======")
    fitted <- c(fitted, pred_list)
  }
  return (list(Score=roc(y,fitted)$auc, Pred=0))
}
# t <- rf_objective(500, 2)
bayesian_optimized_rf <- BayesianOptimization(FUN=rf_objective, bounds=list(ntree=c(500, 750), mtry=c(1,4)), init_points=10, n_iter=20)
```
As you can see above, randomForests from the ranger class as a very poor AUC. 

We will now try the xgboost package with bayesian optimization
```{r}
data <- data.train_no_labels
y <- data.train_labels #droplevels(iris[51:150,5])
b <- sample(1:100) 
data <- data[b,]
y <- y[b]

xgboost_objective <- function(eta) {
  n <- nrow(data)
  splits <- c(seq(1,n, n/10),n+1)
  jumps <- n/10
  fitted <- c()
  for (i in 1:10) {
    test <- splits[i:(i+1)]
    test[2] <- test[2]-1
    test.y <- as.factor(y[test[1]: test[2]])
    # print(test.y)
    test.data <- data.frame(y=test.y, data[test[1]:test[2],])
    
    # print("test.y")
    # print(as.numeric(as.character(test.y)))
    
    train <- (1:n)[-(test[1]:test[2])]
    train.y <- as.factor(y[-(test[1]:test[2])])
    train.data <- data.frame(y=train.y,data[train,])
    
    # print("train.y")
    # print(as.numeric(as.character(train.y)))
    
    # Convert the data to xgboost format
    train_matrix <- xgb.DMatrix(data=as.matrix(data[train,]), label=as.numeric(as.character(train.y)))
    test_matrix <- xgb.DMatrix(data=as.matrix(data[test[1]:test[2],], label=as.numeric(as.character(test.y))))
    params <- list(
      objective = "binary:logistic",
      eval_metric = "logloss",
      max_depth = 40,
      eta = eta,
      nthread = 4
    )
    
    fit <- xgboost(data = train_matrix, params = params, nrounds = 50, verbose = 0)
    pred <- predict(fit, test_matrix)
    pred_list <- ifelse(pred>0.5, 1, 0)
    # print("=======")
    # print(as.numeric(as.character(pred$predictions)))
    # print(pred_list)
    # print("=======")
    fitted <- c(fitted, pred_list)
  }
  return (list(Score=roc(y,fitted)$auc, Pred=0))
}
# t <- xgboost_objective(500, 2)
bayesian_optimized_rf <- BayesianOptimization(FUN=xgboost_objective, bounds=list(eta=c(0.0001,1)), init_points=100, n_iter=20)
```
For some reason, the above model does not get above "0.63"

Since we found that the svm model did the best on our training and cross validation sets, we decided to use this with our imputed data 

Using the imputed data:
```{r}
library(mice)
num_imp = 1
data.train <- read.csv("cs-training.csv")
data.test <- read.csv("cs-test.csv")

imputed_data_train <- mice(data.train, m=num_imp, method='pmm', seed=47, printFlag=FALSE)
imputed_data_test <- mice(data.test, m=num_imp, method='pmm', seed=47, printFlag=FALSE)
```

```{r}
library(e1071)
svm.test <- function(gamma, cost) {
  svm.models = c()
  for(i in 1:num_imp) {
    print(i)
    svm.models[[i]] <- svm(factor(SeriousDlqin2yrs) ~ . - X, data = complete(imputed_data_train, i), 
                           gamma=gamma, 
                           cost=cost)
  }

  predictions <- lapply(1:num_imp, function(m) {
    print(m)
    predict(svm.models[[m]], newdata = complete(imputed_data_test, m))
  })
  
  binaryPredictions <- lapply(predictions, function(x) ifelse(x > 0.5, 1, 0))
  finalPredictions <- apply(do.call(cbind, binaryPredictions), 1, function(x) {
      names(which.max(table(x)))
  })
  return (finalPredictions)
}

test <- svm.test(0.377, 508)
```

```{r}
test
```


Ignore for now:

```{r}
data_imputed_no_labels <- imputed_data$data
column_to_remove <- "X"
column_to_remove_2 <- "SeriousDlqin2yrs"
data_imputed <- imputed_data$data
data_imputed <- data_imputed[,-which(names(data_imputed_no_labels) == column_to_remove)]
data_imputed_no_labels <- data_imputed_no_labels[,-which(names(data_imputed_no_labels) == column_to_remove)]
data_imputed_no_labels <- data_imputed_no_labels[,-which(names(data_imputed_no_labels) == column_to_remove_2)]
y_imputed <- imputed_data$data$SeriousDlqin2yrs #droplevels(iris[51:150,5])
print(data_imputed)
# print(y_imputed)
```
Next steps:
Figure out why the accuracies are exactly the same (they shouldn't be)
Figure out a way to tune these classifiers with cross validation

```{r}

```


